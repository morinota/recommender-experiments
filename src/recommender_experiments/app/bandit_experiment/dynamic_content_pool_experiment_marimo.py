"""å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ—ãƒ¼ãƒ«ã§ã®ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ€§èƒ½å®Ÿé¨“ï¼ˆmarimoæœ€é©åŒ–ç‰ˆï¼‰"""

import marimo

__generated_with = "0.15.2"
app = marimo.App(width="medium")


@app.cell
def _():
    import marimo as mo

    return (mo,)


@app.cell
def _(mo):
    mo.md(
        r"""
    # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ—ãƒ¼ãƒ«ã§ã®ãƒˆãƒ³ãƒ—ã‚½ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ€§èƒ½å®Ÿé¨“

    ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€æ¨è–¦å€™è£œã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ—ãƒ¼ãƒ«ãŒå‹•çš„ã«å¤‰åŒ–ã™ã‚‹ç’°å¢ƒã«ãŠã„ã¦ã€
    Context-free Thompson Samplingã¨Contextual Thompson Samplingã®æ€§èƒ½ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚

    ## å®Ÿé¨“æ¦‚è¦
    - **å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ—ãƒ¼ãƒ«**: å®Ÿé¨“æœŸé–“ä¸­ã«actionã‚»ãƒƒãƒˆãŒ3æ®µéšã§å¤‰åŒ–
    - **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¯”è¼ƒ**: Context-free vs Contextual Thompson Sampling
    - **è©•ä¾¡æŒ‡æ¨™**: ç´¯ç©å ±é…¬ã€ç¬æ™‚å ±é…¬ã€æ®µéšåˆ¥æ€§èƒ½åˆ†æ
    """
    )
    return


@app.cell
def _(mo):
    # ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from typing import Dict, List, Tuple

    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    import seaborn as sns

    from recommender_experiments.service.algorithms.bandit_algorithm_interface import OnlineEvaluationResults
    from recommender_experiments.service.algorithms.thompson_sampling_contextfree import ThompsonSamplingContextFree
    from recommender_experiments.service.algorithms.thompson_sampling_ranking import ThompsonSamplingRanking
    from recommender_experiments.service.environment.ranking_synthetic_dataset import RankingSyntheticBanditDataset

    # å¯è¦–åŒ–ã®è¨­å®š
    plt.style.use("seaborn-whitegrid")
    sns.set_palette("husl")
    plt.rcParams["figure.figsize"] = (12, 8)
    plt.rcParams["font.size"] = 10

    mo.md("âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")
    return (
        Dict,
        List,
        OnlineEvaluationResults,
        RankingSyntheticBanditDataset,
        ThompsonSamplingContextFree,
        ThompsonSamplingRanking,
        np,
        pd,
        plt,
    )


@app.cell
def _(mo):
    # å®Ÿé¨“è¨­å®šã®UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
    mo.md("## ğŸ›ï¸ å®Ÿé¨“è¨­å®š")
    return


@app.cell
def _(mo):
    # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªå®Ÿé¨“è¨­å®š
    num_trials_slider = mo.ui.slider(start=100, stop=2000, step=100, value=1000, label="å®Ÿé¨“è©¦è¡Œæ•°")

    num_actions_initial_slider = mo.ui.slider(start=10, stop=30, step=5, value=20, label="åˆæœŸactionæ•°")

    num_actions_total_slider = mo.ui.slider(start=30, stop=100, step=10, value=50, label="æœ€å¤§actionæ•°")

    k_slider = mo.ui.slider(start=2, stop=5, step=1, value=3, label="ãƒ©ãƒ³ã‚­ãƒ³ã‚°é•·")

    dim_context_slider = mo.ui.slider(start=3, stop=10, step=1, value=5, label="ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¬¡å…ƒ")

    settings_form = mo.vstack(
        [num_trials_slider, num_actions_initial_slider, num_actions_total_slider, k_slider, dim_context_slider]
    )

    return (
        dim_context_slider,
        k_slider,
        num_actions_initial_slider,
        num_actions_total_slider,
        num_trials_slider,
        settings_form,
    )


@app.cell
def _(settings_form):
    settings_form
    return


@app.cell
def _(
    dim_context_slider,
    k_slider,
    mo,
    num_actions_initial_slider,
    num_actions_total_slider,
    num_trials_slider,
):
    # è¨­å®šå€¤ã®å–å¾—ã¨è¡¨ç¤º
    RANDOM_STATE = 12345
    NUM_TRIALS = num_trials_slider.value
    NUM_ACTIONS_INITIAL = num_actions_initial_slider.value
    NUM_ACTIONS_TOTAL = num_actions_total_slider.value
    K = k_slider.value
    DIM_CONTEXT = dim_context_slider.value

    config_display = mo.md(f"""
    **ç¾åœ¨ã®å®Ÿé¨“è¨­å®š:**
    - å®Ÿé¨“è©¦è¡Œæ•°: {NUM_TRIALS}
    - åˆæœŸactionæ•°: {NUM_ACTIONS_INITIAL} 
    - æœ€å¤§actionæ•°: {NUM_ACTIONS_TOTAL}
    - ãƒ©ãƒ³ã‚­ãƒ³ã‚°é•·: {K}
    - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¬¡å…ƒ: {DIM_CONTEXT}
    - ä¹±æ•°ã‚·ãƒ¼ãƒ‰: {RANDOM_STATE}
    """)

    config_display
    return (
        DIM_CONTEXT,
        K,
        NUM_ACTIONS_INITIAL,
        NUM_ACTIONS_TOTAL,
        NUM_TRIALS,
        RANDOM_STATE,
    )


@app.cell
def _(Dict, List, NUM_ACTIONS_INITIAL, NUM_ACTIONS_TOTAL, NUM_TRIALS):
    # actionå¤‰æ›´ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ
    def create_action_churn_schedule(
        num_trials: int, num_actions_initial: int, num_actions_total: int
    ) -> Dict[int, List[int]]:
        """å‹•çš„ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ—ãƒ¼ãƒ«ãŒå¤‰åŒ–ã™ã‚‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½œæˆã™ã‚‹"""
        schedule = {}

        # ç¬¬1æ®µéš: åˆæœŸã®actionã‚»ãƒƒãƒˆ (0 - num_trials//3)
        stage1_end = num_trials // 3
        schedule[0] = list(range(num_actions_initial))

        # ç¬¬2æ®µéš: ä¸€éƒ¨actionãŒå‰Šé™¤ã•ã‚Œã€æ–°ã—ã„actionãŒè¿½åŠ  (num_trials//3 - 2*num_trials//3)
        stage2_end = 2 * num_trials // 3
        remaining_initial = list(range(num_actions_initial // 2, num_actions_initial))
        new_actions = list(range(num_actions_initial, num_actions_initial + 10))
        schedule[stage1_end] = remaining_initial + new_actions

        # ç¬¬3æ®µéš: ã•ã‚‰ã«å¤šãã®æ–°ã—ã„actionãŒè¿½åŠ ã•ã‚Œã‚‹ (2*num_trials//3 - num_trials)
        more_new_actions = list(range(num_actions_initial + 10, num_actions_total))
        schedule[stage2_end] = remaining_initial + new_actions + more_new_actions

        return schedule

    action_churn_schedule = create_action_churn_schedule(NUM_TRIALS, NUM_ACTIONS_INITIAL, NUM_ACTIONS_TOTAL)

    return (action_churn_schedule,)


@app.cell
def _(action_churn_schedule, mo, pd):
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è¡¨ç¤º
    schedule_info = []
    for trial_start, actions in action_churn_schedule.items():
        schedule_stage_end = min(
            [t for t in action_churn_schedule.keys() if t > trial_start]
            + [list(action_churn_schedule.keys())[-1] + 100]
        )
        schedule_info.append(
            {
                "é–‹å§‹Trial": trial_start,
                "Actionæ•°": len(actions),
                "Actionä¾‹": str(actions[:5]) + ("..." if len(actions) > 5 else ""),
            }
        )

    schedule_df = pd.DataFrame(schedule_info)

    mo.vstack([mo.md("## ğŸ“… Actionå¤‰æ›´ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"), mo.ui.table(schedule_df)])
    return


@app.cell
def _(
    DIM_CONTEXT,
    K,
    NUM_ACTIONS_TOTAL,
    RANDOM_STATE,
    RankingSyntheticBanditDataset,
    action_churn_schedule,
    np,
):
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç’°å¢ƒä½œæˆ
    def create_dataset_environment(
        action_churn_schedule_param: dict, num_actions_total: int
    ) -> RankingSyntheticBanditDataset:
        """å®Ÿé¨“ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç’°å¢ƒã‚’ä½œæˆã™ã‚‹"""
        np.random.seed(RANDOM_STATE)

        # actionç‰¹å¾´é‡ã®ç”Ÿæˆ
        action_context = np.random.randn(num_actions_total, DIM_CONTEXT)

        # æœŸå¾…å ±é…¬é–¢æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
        theta = np.random.randn(DIM_CONTEXT, num_actions_total) * 0.5
        quadratic_weights = np.random.randn(DIM_CONTEXT, num_actions_total) * 0.2
        action_bias = np.random.randn(num_actions_total, 1) * 0.1
        position_interaction_weights = np.random.randn(K, K) * 0.1

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç’°å¢ƒã‚’ä½œæˆ
        dataset_env = RankingSyntheticBanditDataset(
            dim_context=DIM_CONTEXT,
            num_actions=num_actions_total,
            k=K,
            action_context=action_context,
            theta=theta,
            quadratic_weights=quadratic_weights,
            action_bias=action_bias,
            position_interaction_weights=position_interaction_weights,
            beta=1.0,  # softmaxæ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            reward_noise=0.1,
            random_state=RANDOM_STATE,
            action_churn_schedule=action_churn_schedule_param,
        )

        return dataset_env

    dataset_env = create_dataset_environment(action_churn_schedule, NUM_ACTIONS_TOTAL)
    return (dataset_env,)


@app.cell
def _(
    Dict,
    K,
    List,
    OnlineEvaluationResults,
    RankingSyntheticBanditDataset,
    np,
):
    # å®Ÿé¨“å®Ÿè¡Œé–¢æ•°
    def run_online_bandit_experiment(
        dataset_env: RankingSyntheticBanditDataset,
        algorithm_name: str,
        algorithm_instance,
        num_trials: int,
        action_churn_schedule_param: Dict[int, List[int]],
    ) -> OnlineEvaluationResults:
        """ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒãƒ³ãƒ‡ã‚£ãƒƒãƒˆå®Ÿé¨“ã‚’å®Ÿè¡Œã™ã‚‹"""
        results = OnlineEvaluationResults(algorithm_name)
        algorithm_instance.reset()

        for trial in range(num_trials):
            # 1å›åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
            synthetic_data = dataset_env.obtain_batch_bandit_feedback(1)

            context = synthetic_data.context_features[0]
            available_actions = np.where(synthetic_data.available_action_mask[0] == 1)[0]

            # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§actioné¸æŠ
            selected_actions = algorithm_instance.select_actions(context, available_actions, K)

            # æœ€é©actionã¨å ±é…¬è¨ˆç®—
            available_q_values = synthetic_data.base_q_function[0, available_actions]
            optimal_actions_idx = np.argsort(available_q_values)[-K:][::-1]
            optimal_actions = available_actions[optimal_actions_idx]

            # å®Ÿéš›ã®å ±é…¬å–å¾—
            selected_rewards = [
                synthetic_data.base_q_function[0, action_id]
                for action_id in selected_actions
                if action_id < len(synthetic_data.base_q_function[0])
            ]
            optimal_rewards = [
                synthetic_data.base_q_function[0, action_id] for i, action_id in enumerate(optimal_actions) if i < K
            ]

            # regretè¨ˆç®—
            instant_reward = sum(selected_rewards) if selected_rewards else 0.0
            optimal_reward_sum = sum(optimal_rewards) if optimal_rewards else 0.0
            instant_regret = optimal_reward_sum - instant_reward

            # å­¦ç¿’æ›´æ–°
            algorithm_instance.update(context, selected_actions, selected_rewards)

            # çµæœè¨˜éŒ²
            results.add_trial_result(selected_actions, instant_regret, instant_reward)

            # é€²æ—è¡¨ç¤º
            if (trial + 1) % 200 == 0:
                print(f"  Trial {trial + 1}: å¹³å‡å ±é…¬ = {results.get_average_reward():.4f}")

        return results

    return (run_online_bandit_experiment,)


@app.cell
def _(mo):
    # å®Ÿé¨“å®Ÿè¡Œé–‹å§‹
    mo.md("## ğŸ§ª å®Ÿé¨“å®Ÿè¡Œ")
    return


@app.cell
def _(
    DIM_CONTEXT,
    K,
    NUM_ACTIONS_TOTAL,
    NUM_TRIALS,
    RANDOM_STATE,
    ThompsonSamplingContextFree,
    ThompsonSamplingRanking,
    action_churn_schedule,
    dataset_env,
    mo,
    run_online_bandit_experiment,
):
    # å®Ÿé¨“å®Ÿè¡Œé–‹å§‹
    mo.md("â³ å®Ÿé¨“å®Ÿè¡Œä¸­...")

    print("ğŸ§ª å®Ÿé¨“é–‹å§‹")

    # Context-free Thompson Sampling
    print("ğŸš€ Context-free Thompson Samplingå®Ÿé¨“é–‹å§‹...")
    ts_contextfree = ThompsonSamplingContextFree(
        num_actions=NUM_ACTIONS_TOTAL, k=K, alpha=1.0, beta=1.0, random_state=RANDOM_STATE
    )

    results_contextfree = run_online_bandit_experiment(
        dataset_env, "Thompson Sampling (Context-free)", ts_contextfree, NUM_TRIALS, action_churn_schedule
    )
    print(f"âœ… Context-freeå®Œäº†: ç´¯ç©å ±é…¬ {results_contextfree.get_final_cumulative_reward():.2f}")

    # Contextual Thompson Sampling
    print("ğŸš€ Contextual Thompson Samplingå®Ÿé¨“é–‹å§‹...")
    ts_contextual = ThompsonSamplingRanking(
        num_actions=NUM_ACTIONS_TOTAL, k=K, dim_context=DIM_CONTEXT, alpha=1.0, beta=1.0, random_state=RANDOM_STATE
    )

    results_contextual = run_online_bandit_experiment(
        dataset_env, "Thompson Sampling (Contextual)", ts_contextual, NUM_TRIALS, action_churn_schedule
    )
    print(f"âœ… Contextualå®Œäº†: ç´¯ç©å ±é…¬ {results_contextual.get_final_cumulative_reward():.2f}")

    all_results = [results_contextfree, results_contextual]

    print("âœ… å®Ÿé¨“å®Œäº†!")
    mo.md("âœ… å®Ÿé¨“å®Œäº†!")
    return all_results, results_contextfree, results_contextual


@app.cell
def _(all_results, mo, pd):
    # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
    summary_data = []
    for summary_result in all_results:
        summary_data.append(
            {
                "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ": summary_result.algorithm_name,
                "æœ€çµ‚ç´¯ç©å ±é…¬": f"{summary_result.get_final_cumulative_reward():.2f}",
                "å¹³å‡å ±é…¬": f"{summary_result.get_average_reward():.4f}",
                "å¹³å‡Regret": f"{summary_result.get_average_regret():.4f}",
            }
        )

    summary_df = pd.DataFrame(summary_data)

    mo.vstack([mo.md("## ğŸ“Š å®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼"), mo.ui.table(summary_df)])
    return


@app.cell
def _(action_churn_schedule, all_results, pd, plt):
    # ãƒ¡ã‚¤ãƒ³çµæœå¯è¦–åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    # 1. ç´¯ç©å ±é…¬ã®æ¨ç§» (ä¸»è¦æŒ‡æ¨™)
    ax1 = axes[0, 0]
    for plot_result in all_results:
        ax1.plot(plot_result.cumulative_reward, label=plot_result.algorithm_name, alpha=0.8, linewidth=2)
    ax1.set_xlabel("Trial")
    ax1.set_ylabel("Cumulative Reward")
    ax1.set_title("ç´¯ç©å ±é…¬ã®æ¨ç§»")
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. ç´¯ç©Regretã®æ¨ç§» (å‚è€ƒæŒ‡æ¨™)
    ax2 = axes[0, 1]
    for plot_result in all_results:
        ax2.plot(plot_result.cumulative_regret, label=plot_result.algorithm_name, alpha=0.8, linewidth=2)
    ax2.set_xlabel("Trial")
    ax2.set_ylabel("Cumulative Regret")
    ax2.set_title("ç´¯ç©Regretã®æ¨ç§» (å‚è€ƒ)")
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 3. ç¬æ™‚Regretã®ç§»å‹•å¹³å‡
    ax3 = axes[1, 0]
    window_size = 50
    for plot_result in all_results:
        instant_regret_series = pd.Series(plot_result.instant_regret)
        moving_avg = instant_regret_series.rolling(window=window_size, center=True).mean()
        ax3.plot(moving_avg, label=f"{plot_result.algorithm_name} (MA={window_size})", alpha=0.8, linewidth=2)
    ax3.set_xlabel("Trial")
    ax3.set_ylabel("Instant Regret (Moving Average)")
    ax3.set_title("ç¬æ™‚Regretã®ç§»å‹•å¹³å‡")
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # 4. actionå¤‰æ›´ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®å¯è¦–åŒ–
    ax4 = axes[1, 1]

    # actionå¤‰æ›´ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’ç¸¦ç·šã§è¡¨ç¤º
    change_points = list(action_churn_schedule.keys())
    colors = ["red", "blue", "green"]
    for i, change_point in enumerate(change_points):
        if i < len(colors):
            ax4.axvline(
                x=change_point,
                color=colors[i],
                linestyle="--",
                alpha=0.7,
                linewidth=2,
                label=f"Stage {i + 1} (Actions: {len(action_churn_schedule[change_point])})",
            )

    # ç¬æ™‚Regretã‚’backgroundã¨ã—ã¦è¡¨ç¤º
    for plot_result in all_results:
        instant_regret_series = pd.Series(plot_result.instant_regret)
        moving_avg = instant_regret_series.rolling(window=window_size, center=True).mean()
        ax4.plot(moving_avg, alpha=0.3, linewidth=1, label=f"{plot_result.algorithm_name}")

    ax4.set_xlabel("Trial")
    ax4.set_ylabel("Instant Regret")
    ax4.set_title("Actionå¤‰æ›´ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã¨æ€§èƒ½ã¸ã®å½±éŸ¿")
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    fig
    return


@app.cell
def _(
    Dict,
    List,
    OnlineEvaluationResults,
    action_churn_schedule,
    all_results,
    mo,
    np,
    pd,
):
    # æ®µéšåˆ¥æ€§èƒ½åˆ†æ
    def analyze_performance_by_stage(
        results: OnlineEvaluationResults, action_churn_schedule_param: Dict[int, List[int]]
    ) -> pd.DataFrame:
        """æ®µéšåˆ¥ã®æ€§èƒ½ã‚’åˆ†æã™ã‚‹"""
        stages = []
        change_points = sorted(action_churn_schedule_param.keys())

        for i in range(len(change_points)):
            stage_start = change_points[i]
            stage_end = change_points[i + 1] if i + 1 < len(change_points) else len(results.instant_regret)

            stage_regrets = results.instant_regret[stage_start:stage_end]
            stage_rewards = results.instant_reward[stage_start:stage_end]

            stages.append(
                {
                    "Stage": i + 1,
                    "é–‹å§‹Trial": stage_start,
                    "çµ‚äº†Trial": stage_end,
                    "Actionæ•°": len(action_churn_schedule_param[stage_start]),
                    "å¹³å‡å ±é…¬": np.mean(stage_rewards) if stage_rewards else 0,
                    "å¹³å‡Regret": np.mean(stage_regrets) if stage_regrets else 0,
                    "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ": results.algorithm_name,
                }
            )

        return pd.DataFrame(stages)

    # å„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ®µéšåˆ¥åˆ†æ
    stage_analysis_list = []
    for stage_result in all_results:
        stage_df = analyze_performance_by_stage(stage_result, action_churn_schedule)
        stage_analysis_list.append(stage_df)

    combined_stage_analysis = pd.concat(stage_analysis_list, ignore_index=True)

    mo.vstack([mo.md("## ğŸ“ˆ æ®µéšåˆ¥æ€§èƒ½åˆ†æ"), mo.ui.table(combined_stage_analysis.round(4))])
    return (combined_stage_analysis,)


@app.cell
def _(combined_stage_analysis, plt):
    # æ®µéšåˆ¥æ€§èƒ½ã®å¯è¦–åŒ–
    fig2, axes2 = plt.subplots(1, 2, figsize=(16, 6))

    # æ®µéšåˆ¥å¹³å‡å ±é…¬ (ä¸»è¦æŒ‡æ¨™)
    ax2_1 = axes2[0]
    pivot_reward = combined_stage_analysis.pivot(index="Stage", columns="ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ", values="å¹³å‡å ±é…¬")
    pivot_reward.plot(kind="bar", ax=ax2_1, alpha=0.8, width=0.7)
    ax2_1.set_xlabel("Stage")
    ax2_1.set_ylabel("Average Reward")
    ax2_1.set_title("æ®µéšåˆ¥å¹³å‡å ±é…¬")
    ax2_1.legend(title="ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ", bbox_to_anchor=(1.05, 1), loc="upper left")
    ax2_1.grid(True, alpha=0.3)
    ax2_1.set_xticklabels([f"Stage {i}" for i in range(1, 4)], rotation=0)

    # æ®µéšåˆ¥å¹³å‡Regret (å‚è€ƒæŒ‡æ¨™)
    ax2_2 = axes2[1]
    pivot_regret = combined_stage_analysis.pivot(index="Stage", columns="ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ", values="å¹³å‡Regret")
    pivot_regret.plot(kind="bar", ax=ax2_2, alpha=0.8, width=0.7)
    ax2_2.set_xlabel("Stage")
    ax2_2.set_ylabel("Average Regret")
    ax2_2.set_title("æ®µéšåˆ¥å¹³å‡Regret (å‚è€ƒ)")
    ax2_2.legend(title="ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ", bbox_to_anchor=(1.05, 1), loc="upper left")
    ax2_2.grid(True, alpha=0.3)
    ax2_2.set_xticklabels([f"Stage {i}" for i in range(1, 4)], rotation=0)

    plt.tight_layout()
    fig2
    return


@app.cell
def _(
    Dict,
    List,
    OnlineEvaluationResults,
    action_churn_schedule,
    all_results,
    mo,
    pd,
):
    # Actioné¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    def analyze_action_selection_patterns(
        results: OnlineEvaluationResults, action_churn_schedule_param: Dict[int, List[int]]
    ) -> dict:
        """actioné¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã™ã‚‹"""
        pattern_analysis = {}
        change_points = sorted(action_churn_schedule_param.keys())

        for i in range(len(change_points)):
            stage_start = change_points[i]
            stage_end = change_points[i + 1] if i + 1 < len(change_points) else len(results.selected_actions_history)

            stage_actions = results.selected_actions_history[stage_start:stage_end]
            available_actions = action_churn_schedule_param[stage_start]

            # å„actionã®é¸æŠé »åº¦ã‚’è¨ˆç®—
            action_counts = {}
            total_selections = 0

            for trial_actions in stage_actions:
                for action_id in trial_actions:
                    action_counts[action_id] = action_counts.get(action_id, 0) + 1
                    total_selections += 1

            # åˆ©ç”¨å¯èƒ½actionã®ã¿ã®é¸æŠç‡ã‚’è¨ˆç®—
            available_action_rates = {}
            for action_id in available_actions:
                rate = action_counts.get(action_id, 0) / max(total_selections, 1)
                available_action_rates[action_id] = rate

            pattern_analysis[f"Stage_{i + 1}"] = {
                "available_actions": available_actions,
                "action_selection_rates": available_action_rates,
                "diversity_score": len([r for r in available_action_rates.values() if r > 0.01])
                / len(available_actions),
            }

        return pattern_analysis

    # Actioné¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã®å®Ÿè¡Œã¨è¡¨ç¤º
    pattern_results = []
    for pattern_result in all_results:
        patterns = analyze_action_selection_patterns(pattern_result, action_churn_schedule)
        for stage_name, pattern in patterns.items():
            sorted_rates = sorted(pattern["action_selection_rates"].items(), key=lambda x: x[1], reverse=True)
            top_actions = sorted_rates[:3]  # ãƒˆãƒƒãƒ—3ã®ã¿è¡¨ç¤º

            pattern_results.append(
                {
                    "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ": pattern_result.algorithm_name,
                    "Stage": stage_name.replace("Stage_", ""),
                    "Diversity Score": f"{pattern['diversity_score']:.3f}",
                    "Top3 Actions": str([(action_id, f"{rate:.3f}") for action_id, rate in top_actions]),
                }
            )

    pattern_df = pd.DataFrame(pattern_results)

    mo.vstack([mo.md("## ğŸ¯ Actioné¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"), mo.ui.table(pattern_df)])
    return


@app.cell
def _(
    DIM_CONTEXT,
    K,
    NUM_TRIALS,
    action_churn_schedule,
    mo,
    results_contextfree,
    results_contextual,
):
    # æœ€çµ‚ã¾ã¨ã‚
    better_algorithm = (
        "Context-free"
        if results_contextfree.get_final_cumulative_reward() > results_contextual.get_final_cumulative_reward()
        else "Contextual"
    )

    stages_info = ""
    for trial_start_idx, action_ids in action_churn_schedule.items():
        stage_end = min([t for t in action_churn_schedule.keys() if t > trial_start_idx] + [NUM_TRIALS])
        stages_info += f"- Trial {trial_start_idx}-{stage_end - 1}: {len(action_ids)}å€‹ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„\n"

    final_summary = mo.md(f"""
    ## ğŸ‰ å®Ÿé¨“çµæœã¾ã¨ã‚

    ### å®Ÿé¨“è¨­å®š
    - **ç·è©¦è¡Œæ•°**: {NUM_TRIALS}
    - **ãƒ©ãƒ³ã‚­ãƒ³ã‚°é•·**: {K}
    - **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¬¡å…ƒ**: {DIM_CONTEXT}
    - **ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ—ãƒ¼ãƒ«å¤‰åŒ–**:
    {stages_info}

    ### ä¸»è¦ãªç™ºè¦‹
    - **å„ªç§€ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: {better_algorithm} Thompson Sampling
    - **Context-free**: ç´¯ç©å ±é…¬ {results_contextfree.get_final_cumulative_reward():.2f}
    - **Contextual**: ç´¯ç©å ±é…¬ {results_contextual.get_final_cumulative_reward():.2f}

    ### è€ƒå¯Ÿ
    {"- ã‚·ãƒ³ãƒ—ãƒ«ãªContext-freeã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå‹•çš„ç’°å¢ƒã§é ‘å¥æ€§ã‚’ç™ºæ®" if better_algorithm == "Context-free" else "- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ãŒå‹•çš„ç’°å¢ƒã§ã®é©å¿œã«æœ‰åŠ¹"}
    - ä¸¡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã‚‚ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ—ãƒ¼ãƒ«å¤‰åŒ–ã«é©å¿œ
    - æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¿½åŠ ã•ã‚Œã‚‹éš›ã®æ¢ç´¢-æ´»ç”¨ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒé‡è¦
    """)

    final_summary
    return


if __name__ == "__main__":
    app.run()
