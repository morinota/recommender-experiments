{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81165deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import check_random_state\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "y_label_dict = {\"se\": \"左図：平均二乗誤差\", \"bias\": \"中図：二乗バイアス\", \"variance\": \"右図：バリアンス\"}\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import (\n",
    "    SyntheticBanditDatasetWithActionEmbeds as SyntheticBanditDataset,\n",
    "    logistic_polynomial_reward_function,\n",
    ")\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation,\n",
    "    RegressionModel,\n",
    "    InverseProbabilityWeighting as IPS,\n",
    "    DirectMethod as DM,\n",
    "    DoublyRobust as DR,\n",
    ")\n",
    "from utils import eps_greedy_policy, aggregate_simulation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ede89b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.7\n"
     ]
    }
   ],
   "source": [
    "print(obp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc4d6e4",
   "metadata": {},
   "source": [
    "## (データ収集方策が収集した)ログデータのサイズを変化させたときのDM・IPS・DR推定量の平均二乗誤差・二乗バイアス・バリアンスの挙動\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d9714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## シミュレーション設定\n",
    "num_runs = 500  # シミュレーションの繰り返し回数\n",
    "dim_context = 10  # 特徴量xの次元\n",
    "n_actions = 20  # 行動数, |A|\n",
    "beta = -3  # データ収集方策のパラメータ\n",
    "test_data_size = 100000  # 評価方策の真の性能を近似するためのテストデータのサイズ\n",
    "random_state = 12345\n",
    "random_ = check_random_state(random_state)\n",
    "num_data_list = [250, 500, 1000, 2000, 4000, 8000]  # データ収集方策が収集したログデータのサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc10742",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_list = []\n",
    "for num_data in tqdm(num_data_list):\n",
    "    ## 合成データセット生成クラス\n",
    "    dataset = SyntheticBanditDataset(\n",
    "        n_actions=n_actions,\n",
    "        dim_context=dim_context,\n",
    "        action_context=random_.normal(size=(n_actions, 10)),  # 行動の特徴量\n",
    "        beta=beta,  # データ収集方策のパラメータ(大きいほど一様に近い)\n",
    "        reward_function=logistic_polynomial_reward_function,  # 報酬関数\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    ## 評価方策の真の性能(policy value)を近似するためのテストデータ生成\n",
    "    test_data = dataset.obtain_batch_bandit_feedback(n_rounds=test_data_size)\n",
    "\n",
    "    ## 評価方策の真の性能(policy value)を近似\n",
    "    policy_value = dataset.calc_ground_truth_policy_value(\n",
    "        expected_reward=test_data[\"expected_reward\"],\n",
    "        action_dist=eps_greedy_policy(test_data[\"extended_reward\"]),\n",
    "    )\n",
    "\n",
    "    estimated_policy_value_list = []\n",
    "    for _ in tqdm(range(num_runs), desc=f\"num_data: {num_data}\"):\n",
    "        ## データ収集方策が形成する分布に従い、ログデータを生成\n",
    "        offline_logged_data = dataset.obtain_batch_bandit_feedback(n_rounds=num_data)\n",
    "\n",
    "        ## ログデータ上における評価方策の行動選択確率を計算\n",
    "        pi = eps_greedy_policy(offline_logged_data[\"extended_reward\"])\n",
    "\n",
    "        ## 期待報酬関数に対する推定モデル \\hat{q}(x,a) を得る\n",
    "        reg_model = RegressionModel(\n",
    "            n_actions=dataset.n_actions,\n",
    "            base_model=LogisticRegression(C=100, random_state=random_state),\n",
    "        )\n",
    "        estimated_rewards_lr = reg_model.fit_predict(\n",
    "            context=offline_logged_data[\"context\"],  # context; x\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender-experiments-_FVBVT8O-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
