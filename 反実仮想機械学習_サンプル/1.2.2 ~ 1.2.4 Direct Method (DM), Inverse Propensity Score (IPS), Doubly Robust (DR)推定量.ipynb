{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81165deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import check_random_state\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "y_label_dict = {\"se\": \"左図：平均二乗誤差\", \"bias\": \"中図：二乗バイアス\", \"variance\": \"右図：バリアンス\"}\n",
    "\n",
    "# import open bandit pipeline (obp)\n",
    "import obp\n",
    "from obp.dataset import (\n",
    "    SyntheticBanditDatasetWithActionEmbeds as SyntheticBanditDataset,\n",
    "    logistic_polynomial_reward_function,\n",
    ")\n",
    "from obp.ope import (\n",
    "    OffPolicyEvaluation,\n",
    "    RegressionModel,\n",
    "    InverseProbabilityWeighting as IPS,\n",
    "    DirectMethod as DM,\n",
    "    DoublyRobust as DR,\n",
    ")\n",
    "from utils import eps_greedy_policy, aggregate_simulation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ede89b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.7\n"
     ]
    }
   ],
   "source": [
    "print(obp.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc4d6e4",
   "metadata": {},
   "source": [
    "## (データ収集方策が収集した)ログデータのサイズを変化させたときのDM・IPS・DR推定量の平均二乗誤差・二乗バイアス・バリアンスの挙動\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d9714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## シミュレーション設定\n",
    "num_runs = 500  # シミュレーションの繰り返し回数\n",
    "dim_context = 10  # 特徴量xの次元\n",
    "n_actions = 20  # 行動数, |A|\n",
    "beta = 1  # データ収集方策のパラメータ（-3から1に変更：より安定的な値）\n",
    "test_data_size = 100000  # 評価方策の真の性能を近似するためのテストデータのサイズ\n",
    "random_state = 12345\n",
    "random_ = check_random_state(random_state)\n",
    "num_data_list = [250, 500, 1000, 2000, 4000, 8000]  # データ収集方策が収集したログデータのサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cc10742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_data=250...:   6%|▌         | 31/500 [00:03<00:51,  9.13it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "p < 0, p > 1 or p contains NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m estimated_policy_value_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_runs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_data=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m## データ収集方策が形成する分布に従いログデータを生成\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     offline_logged_data \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobtain_batch_bandit_feedback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m## ログデータ上における評価方策の行動選択確率を計算\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     pi \u001b[38;5;241m=\u001b[39m eps_greedy_policy(offline_logged_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/recommender-experiments-_FVBVT8O-py3.10/lib/python3.10/site-packages/obp/dataset/synthetic_embed.py:327\u001b[0m, in \u001b[0;36mSyntheticBanditDatasetWithActionEmbeds.obtain_batch_bandit_feedback\u001b[0;34m(self, n_rounds)\u001b[0m\n\u001b[1;32m    322\u001b[0m     expected_rewards_factual \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m         cat_dim_importance[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, d]\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;241m*\u001b[39m q_x_e[np\u001b[38;5;241m.\u001b[39marange(n_rounds), action_embed[:, d], d]\n\u001b[1;32m    325\u001b[0m     )\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RewardType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_type) \u001b[38;5;241m==\u001b[39m RewardType\u001b[38;5;241m.\u001b[39mBINARY:\n\u001b[0;32m--> 327\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_rewards_factual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m RewardType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_type) \u001b[38;5;241m==\u001b[39m RewardType\u001b[38;5;241m.\u001b[39mCONTINUOUS:\n\u001b[1;32m    329\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_\u001b[38;5;241m.\u001b[39mnormal(\n\u001b[1;32m    330\u001b[0m         loc\u001b[38;5;241m=\u001b[39mexpected_rewards_factual, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_std, size\u001b[38;5;241m=\u001b[39mn_rounds\n\u001b[1;32m    331\u001b[0m     )\n",
      "File \u001b[0;32mmtrand.pyx:3389\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.binomial\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:391\u001b[0m, in \u001b[0;36mnumpy.random._common.check_array_constraint\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:377\u001b[0m, in \u001b[0;36mnumpy.random._common._check_array_cons_bounded_0_1\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: p < 0, p > 1 or p contains NaNs"
     ]
    }
   ],
   "source": [
    "result_df_list = []\n",
    "for num_data in num_data_list:\n",
    "    ## 人工データ生成クラス\n",
    "    dataset = SyntheticBanditDataset(\n",
    "        n_actions=n_actions,\n",
    "        dim_context=dim_context,\n",
    "        action_context=random_.normal(size=(n_actions, 10), scale=0.5),  # scaleを0.5に小さくして安定化\n",
    "        beta=beta,\n",
    "        reward_function=logistic_polynomial_reward_function,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    ## 評価方策の真の性能(policy value)を近似するためのテストデータ\n",
    "    test_data = dataset.obtain_batch_bandit_feedback(n_rounds=test_data_size)\n",
    "\n",
    "    ## 評価方策の真の性能(policy value)を近似\n",
    "    policy_value = dataset.calc_ground_truth_policy_value(\n",
    "        expected_reward=test_data[\"expected_reward\"],\n",
    "        action_dist=eps_greedy_policy(test_data[\"expected_reward\"]),\n",
    "    )\n",
    "\n",
    "    estimated_policy_value_list = []\n",
    "    for _ in tqdm(range(num_runs), desc=f\"num_data={num_data}...\"):\n",
    "        ## データ収集方策が形成する分布に従いログデータを生成\n",
    "        offline_logged_data = dataset.obtain_batch_bandit_feedback(n_rounds=num_data)\n",
    "\n",
    "        ## ログデータ上における評価方策の行動選択確率を計算\n",
    "        pi = eps_greedy_policy(offline_logged_data[\"expected_reward\"])\n",
    "\n",
    "        ## 期待報酬関数に対する推定モデル\\hat{q}(x,a)を得る\n",
    "        reg_model = RegressionModel(\n",
    "            n_actions=dataset.n_actions,\n",
    "            base_model=LogisticRegression(C=100, random_state=random_state),\n",
    "        )\n",
    "        estimated_rewards_lr = reg_model.fit_predict(\n",
    "            context=offline_logged_data[\"context\"],  # context; x\n",
    "            action=offline_logged_data[\"action\"],  # action; a\n",
    "            reward=offline_logged_data[\"reward\"],  # reward; r\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        reg_model = RegressionModel(\n",
    "            n_actions=dataset.n_actions,\n",
    "            base_model=MLPClassifier(hidden_layer_sizes=(10, 10), random_state=random_state),\n",
    "        )\n",
    "        estimated_rewards_mlp = reg_model.fit_predict(\n",
    "            context=offline_logged_data[\"context\"],  # context; x\n",
    "            action=offline_logged_data[\"action\"],  # action; a\n",
    "            reward=offline_logged_data[\"reward\"],  # reward; r\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        ## ログデータを用いてオフ方策評価を実行する\n",
    "        ope = OffPolicyEvaluation(\n",
    "            bandit_feedback=offline_logged_data,\n",
    "            ope_estimators=[\n",
    "                IPS(estimator_name=\"IPS\"),\n",
    "                DR(estimator_name=\"DR\"),\n",
    "                DM(estimator_name=\"lr\"),\n",
    "                DM(estimator_name=\"mlp\"),\n",
    "            ],\n",
    "        )\n",
    "        estimated_policy_values = ope.estimate_policy_values(\n",
    "            action_dist=pi,  # \\pi(a|x)\n",
    "            estimated_rewards_by_reg_model={\n",
    "                \"DR\": estimated_rewards_mlp,\n",
    "                \"lr\": estimated_rewards_lr,\n",
    "                \"mlp\": estimated_rewards_mlp,\n",
    "            },\n",
    "        )\n",
    "        estimated_policy_value_list.append(estimated_policy_values)\n",
    "\n",
    "    ## シミュレーション結果を集計する\n",
    "    result_df_list.append(\n",
    "        aggregate_simulation_results(\n",
    "            estimated_policy_value_list,\n",
    "            policy_value,\n",
    "            \"num_data\",\n",
    "            num_data,\n",
    "        )\n",
    "    )\n",
    "result_df = pd.concat(result_df_list).reset_index(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca5177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender-experiments-_FVBVT8O-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
