{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Callable, Optional, TypedDict\n",
    "import numpy as np\n",
    "from obp.dataset import SyntheticBanditDataset, OpenBanditDataset\n",
    "from obp.policy import BernoulliTS\n",
    "from obp.ope import ReplayMethod, InverseProbabilityWeighting, BaseOffPolicyEstimator\n",
    "from obp.ope import OffPolicyEvaluation, InverseProbabilityWeighting as IPW\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditFeedbackDict(TypedDict):\n",
    "    n_rounds: int  # ラウンド数\n",
    "    n_actions: int  # アクション数s\n",
    "    context: np.ndarray  # 文脈 (shape: (n_rounds, dim_context))\n",
    "    action_context: np.ndarray  # アクション特徴量 (shape: (n_actions, dim_action_features))\n",
    "    action: np.ndarray  # 実際に選択されたアクション (shape: (n_rounds,))\n",
    "    position: Optional[np.ndarray]  # ポジション (shape: (n_rounds,) or None)\n",
    "    reward: np.ndarray  # 報酬 (shape: (n_rounds,))\n",
    "    expected_reward: np.ndarray  # 期待報酬 (shape: (n_rounds, n_actions))\n",
    "    pi_b: np.ndarray  # データ収集方策 P(a|x) (shape: (n_rounds, n_actions))\n",
    "    pscore: np.ndarray  # 傾向スコア (shape: (n_rounds,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:obp.dataset.real:When `data_path` is not given, this class downloads the small-sized version of Open Bandit Dataset.\n",
      "INFO:obp.dataset.real:When `data_path` is not given, this class downloads the small-sized version of Open Bandit Dataset.\n",
      "INFO:obp.dataset.real:When `data_path` is not given, this class downloads the small-sized version of Open Bandit Dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rounds: 10000\n",
      "Number of Actions: 80\n",
      "slate size: 3\n",
      "dict_keys(['n_rounds', 'n_actions', 'action', 'position', 'reward', 'pscore', 'context', 'action_context'])\n",
      "0.0042\n",
      "0.0038\n",
      "--------------------\n",
      "Round: 0\n",
      "Context: [1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1]\n",
      "Position: 2\n",
      "Action: 14\n",
      "Action context: [ 5.         10.          4.         -0.49917163]\n",
      "Reward: 0\n",
      "Propensity Score: 0.0125\n",
      "--------------------\n",
      "Round: 1\n",
      "Context: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Position: 2\n",
      "Action: 14\n",
      "Action context: [ 1.         10.          4.         -0.54377537]\n",
      "Reward: 0\n",
      "Propensity Score: 0.0125\n",
      "--------------------\n",
      "Round: 2\n",
      "Context: [1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      "Position: 2\n",
      "Action: 27\n",
      "Action context: [ 1.         12.          1.          0.97275186]\n",
      "Reward: 0\n",
      "Propensity Score: 0.0125\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# 実際にzozotownで収集されたバンディットフィードバックデータの使い方確認\n",
    "\n",
    "# データセットクラスのインスタンス化\n",
    "dataset = OpenBanditDataset(behavior_policy=\"random\", campaign=\"all\")\n",
    "# 属性の確認\n",
    "print(f\"Number of Rounds: {dataset.n_rounds}\")\n",
    "print(f\"Number of Actions: {dataset.n_actions}\")\n",
    "print(f\"slate size: {dataset.len_list}\")\n",
    "\n",
    "# バンディットフィードバックを辞書形式で取得\n",
    "bandit_feedback, _ = dataset.obtain_batch_bandit_feedback(test_size=0.3, is_timeseries_split=True)\n",
    "print(bandit_feedback.keys())\n",
    "\n",
    "# データ収集方策の性能のon-policy評価結果\n",
    "print(f\"{OpenBanditDataset.calc_on_policy_policy_value_estimate('bts', 'all')}\")\n",
    "print(f\"{OpenBanditDataset.calc_on_policy_policy_value_estimate('random', 'all')}\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "# バンディットフィードバックの中身を確認\n",
    "for round_idx in range(3):\n",
    "    print(f\"Round: {round_idx}\")\n",
    "    print(f\"Context: {bandit_feedback['context'][round_idx]}\")\n",
    "    print(f\"Position: {bandit_feedback['position'][round_idx]}\")\n",
    "    print(f\"Action: {bandit_feedback['action'][round_idx]}\")\n",
    "    print(f\"Action context: {bandit_feedback['action_context'][round_idx]}\")\n",
    "    print(f\"Reward: {bandit_feedback['reward'][round_idx]}\")\n",
    "    print(f\"Propensity Score: {bandit_feedback['pscore'][round_idx]}\")\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 80, 3)\n"
     ]
    }
   ],
   "source": [
    "# オフ方策学習\n",
    "evaluation_policy = BernoulliTS(\n",
    "    n_actions=dataset.n_actions,\n",
    "    len_list=dataset.len_list,\n",
    "    is_zozotown_prior=False,\n",
    "    # zozoが用意した事前分布のパラメータを使う場合\n",
    "    # is_zozotown_prior=True,\n",
    "    # campaign=\"all\",\n",
    "    # random_state=12345,\n",
    ")\n",
    "# パラメータ更新\n",
    "for round_idx in range(bandit_feedback[\"n_rounds\"]):\n",
    "    action = bandit_feedback[\"action\"][round_idx]\n",
    "    reward = bandit_feedback[\"reward\"][round_idx]\n",
    "    evaluation_policy.update_params(action=action, reward=reward)\n",
    "action_dist = evaluation_policy.compute_batch_action_dist(n_rounds=bandit_feedback[\"n_rounds\"], n_sim=100000)\n",
    "print(f\"{action_dist.shape}\")  # (n_rounds, n_actions, len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9904571428571427\n"
     ]
    }
   ],
   "source": [
    "# オフ方策評価\n",
    "ope = OffPolicyEvaluation(bandit_feedback=bandit_feedback, ope_estimators=[IPW()])\n",
    "estimated_policy_value = ope.estimate_policy_values(action_dist=action_dist)\n",
    "\n",
    "# オフ方策学習した新方策の性能のオフライン評価値と、データ収集方策の性能のオンライン評価値を比較\n",
    "relative_policy_value_of_bernoulli_ts = estimated_policy_value[\"ipw\"] / bandit_feedback[\"reward\"].mean()\n",
    "print(relative_policy_value_of_bernoulli_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender-experiments-_FVBVT8O-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
